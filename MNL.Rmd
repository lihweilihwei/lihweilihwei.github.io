---
title: "Multinomial Logit Model (MNL)"
output: 
  html_document:
      theme: readable
  
---

### 1 Introduction

This document elaborates on the steps done to run Multinomial Logistic Regression on the results of the <a href="statedpref.html">Stated Preference Survey</a>. Multinomial Logistic Regression serves to predict an individual's mode choice (public bus, shared bicycles/PMDs, or SAAVs) using inputs such as the individual's demographics, mode specific variables and non-mode specific variables.

### 2 The Data

First, we read in the data from the stated preference survey which is in the CSV format (Comma Separated Values).

```{r results = 'hide'}
modechoice <- read.csv("ModeChoice.csv")
```

Looking at the structure of the dataset `str()`, it contains 1000 observations of 40 variables.

```{r results='hide'}
str(modechoice)
```

### 3.1 Predicting "NA" Incomes with Linear Regression

Some of the entries in the `Income` column are `NA`. To fill this missing information, we used linear regression to predict their values.

We separate the dataset into two: one with stated incomes, and another which have `NA` for `Income`.

```{r}
incomenotNA <- subset(modechoice, modechoice$Income != "NA")
incomeNA <- subset(modechoice,is.na(modechoice$Income))
```

There are `r dim(incomenotNA)[1]` observations with stated incomes and `r dim(incomeNA)[1]` observations where income is `NA`. 

### 3.2 Creating Training and Test Sets

We now create training and test sets from the observations where incomes are stated (`incomenotNA`). The `caTools` package is used.

```{r warning=FALSE, message=FALSE}
library(caTools)
```

We set the random seed so that the results will be replicable. The training and test sets are obtained with a 65/35 split ratio.

```{r}
set.seed(1)
split <- sample.split(incomenotNA$Age, SplitRatio = 0.65)

incometraining <- subset(incomenotNA,split==TRUE)
incometest <- subset(incomenotNA,split==FALSE)
```

### 3.3 Creating a Linear Regression Model

We now create a linear regression. The first model includes all demographic information of the individual:

Variable          Description
-------------     -----------------------
`Age`             The individual's age group
`Gender`          The individual's gender
`HouseholdType`   Household type
`HouseholdSize`   Number of people in the household
`Residential`     Residential location of the household
`Cars`            The number of cars in the household
`Concession`      The type of concession pass, if any
-------------     -------------------

```{r}
income.model1 <- lm(Income ~ Age + Gender + HouseholdType + 
                      HouseholdSize + Residential + Cars + 
                      Concession,
                    data = incometraining)

summary(income.model1)
```

The model has an adjusted $R^2$ value of `r round(summary(income.model1)$adj.r.squared, 5)`. `HouseholdType` and `Residential` are not significant at the 90% significance level.

### 3.4 Removing `HouseholdType` and `Residential`

We remove the insignificant variables from the model.

```{r}
income.model2 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession,
                    data=incometraining)

summary(income.model2)
```

This model has an increased adjusted $R^2$ value of `r round(summary(income.model2)$adj.r.squared, 5)`. `Concession` and `(Intercept)` are the least significant variables.

### 3.5 Variable Selection

Several other combinations of variables were tested. The combination of variables which gave the highest adjusted $R^2$ was chosen.

```{r results = 'hide', echo=FALSE}
#without concession
income.model3 <- lm(Income ~ Age + Gender +HouseholdSize + Cars,
                    data=incometraining)

summary(income.model3)
#Adjusted R-squared:  0.3955

#add back concession, and household type
income.model4 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession + HouseholdType,
                    data=incometraining)

summary(income.model4)
#Adjusted R-squared:  0.3979 

cor(modechoice$Residential,modechoice$HouseholdType)

#add back concession, and household type
income.model5 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession + Residential,
                    data=incometraining)

summary(income.model5)
#Adjusted R-squared:  0.398 
```


|Model|`Age`|`Gender`|`HouseholdType`|`HouseholdSize`|`Residential`|`Cars`|`Concession`|Adjusted $R^2$|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---------------|
|1    |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|`r round(summary(income.model1)$adj.r.squared, 5)`|
|**2**|$\checkmark$|$\checkmark$|            |$\checkmark$|            |$\checkmark$|$\checkmark$|**`r round(summary(income.model2)$adj.r.squared, 5)`**|
|3    |$\checkmark$|$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |`r round(summary(income.model3)$adj.r.squared, 5)`|
|4    |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|`r round(summary(income.model4)$adj.r.squared, 5)`|
|5    |$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|`r round(summary(income.model5)$adj.r.squared, 5)`|

Model 2 has the highest adjusted $R^2$. We now evaluate this model on the test set by calculating the test $R^2$. This is given by the formula:

$$R^2 = 1 - \frac{SSE}{SST}$$

where $SSE$  is the Sum of Squared Errors and $SST$ is the Sum of Squared Errors (Total).

```{r}
incomeprediction <- predict(income.model2, newdata=incometest)
sse <- sum((incomeprediction-incometest$Income)^2)
sst <- sum((incometest$Income-mean(incometraining$Income))^2)
```

The test $R^2$ for model 2 is `r round(1-sse/sst, 5)`. We now use model 2 to predict the `NA` entries in the dataset.

```{r}
NAincomeprediction <- predict(income.model2, newdata=incomeNA) 

modechoice$Income <- ifelse(is.na(modechoice$Income),
                            NAincomeprediction,
                            modechoice$Income)
```

Now that all entries in the dataset have no `NA` values, we can proceed to conduct Multinomial Logistic Regression.

### 4.1 Generating Training and Test Sets

Each row in the dataset corresponds to a choice task of a particular individual. We want to randomly split the individuals into the training or test set, while keeping all 8 choice tasks intact.

```{r}
set.seed(0)
mask = rep(x = sample.split(rep(1, 125), SplitRatio = 0.7),
           each = 8)
modechoice.training = subset(modechoice, mask==TRUE)
modechoice.test = subset(modechoice, mask==FALSE)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
```

The table below shows the first 16 rows of the training set. All 8 choice tasks of an individual are preserved. `Case` refers to the individual, while `Task` refers to the choice task.

```{r}
kable(modechoice.training[1:16,2:6])
```

### 4.2 Creating an `mlogit()` Object

We use the `mlogit` package which will help us conduct Multinomial Logistic Regression.

```{r message=FALSE, warning=FALSE}
library(mlogit)
```

```{r}
S <- mlogit.data(modechoice.training,
                 shape="wide",
                 choice="A",
                 varying=c(4:18),
                 sep="",
                 alt.levels=c("A.bus","A.cyclingPMD","A.saav"),
                 id.var="Case")
```


### 4.3 Calculating AIC

The Akaike Information Criterion (AIC) is.... a lower AIC value is better. The formula is given by:

$$AIC = -2 LL(\hat\beta) + 2|\hat\beta|$$

where $LL(\hat\beta)$ is the Log-likelihood of the estimated model and $|\hat\beta|$ is the number of coefficients to be estimated (including the intercept).

For convenience, we define a function `calc.AIC()` that takes in a model and returns its AIC value.

```{r}
calc.AIC <- function (model) {
  return(-2*model$logLik[1] + 2*length(model$coefficients))
}
```



### Blah Blah Blah

```{r eval=FALSE}
library(mlogit)
library(caTools)


#    MODE CHOICE STATED PREFERENCE SURVEY

# The Data ==============================================================


hist(modechoice$Age)

withSAAV <- modechoice[,c(1:21,25:39)]
noSAAV <- modechoice[,c(1:17,22:39)]

#install.packages("mlogit")
library(mlogit)

# Estimating income for refused / unknown using linear regression ============
  
incomenotNA <- subset(modechoice, modechoice$Income != "NA")
str(incomenotNA)
incomeNA <- subset(modechoice,is.na(modechoice$Income))
str(incomeNA)

#install.packages("caTools")

set.seed(1)
split <- sample.split(incomenotNA$Age, SplitRatio = 0.65)
#splits dataset into .65 and .35 with proportion of TENCHD in each being same as it was in framing1

incometraining <- subset(incomenotNA,split==TRUE)
incometest <- subset(incomenotNA,split==FALSE)
str(incometraining) #larger set
str(incometest) #smaller set

#summary(step(income.model1))

income.model1 <- lm(Income ~ Age + Gender + HouseholdType + 
                      HouseholdSize + Residential + Cars + 
                      Concession,
                    data=incometraining)

summary(income.model1)

#Adjusted R-squared:  0.3968
#but household type and residential not sig at all

#take out hsehold type and residential
income.model2 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession,
                    data=incometraining)

summary(income.model2)
#Adjusted R-squared:  0.3991 
#least sig is int and concession

#without concession
income.model3 <- lm(Income ~ Age + Gender +HouseholdSize + Cars,
                    data=incometraining)

summary(income.model3)
#Adjusted R-squared:  0.3955

#add back concession, and household type
income.model4 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession + HouseholdType,
                    data=incometraining)

summary(income.model4)
#Adjusted R-squared:  0.3979 

cor(modechoice$Residential,modechoice$HouseholdType)

#add back concession, and household type
income.model5 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession + Residential,
                    data=incometraining)

summary(income.model5)
#Adjusted R-squared:  0.398 


plot(incomeprediction, jitter(incometest$Income, 1),
     cex = 0.5,
     xlim = c(1,5),
     ylim = c(1, 5))

abline(a=0, b=1)

incomeprediction <- predict(income.model2, newdata=incometest)
sse <- sum((incomeprediction-incometest$Income)^2)
#sse is the sum of square of difference between the true value and the estimated value

sst <- sum((incometest$Income-mean(incometraining$Income))^2)

#sst is the sum of square of diff between the true value and the mean of the true values
1-sse/sst
#this is the R^2
#0.3740837


#cor(modechoice)
# Generating training and test sets ===========================================
library(caTools)

set.seed(0)

mask = rep(x = sample.split(rep(1, 125), SplitRatio = 0.7),
           each = 8)

modechoice.training = subset(modechoice, mask==TRUE)

modechoice.test = subset(modechoice, mask==FALSE)

# ============================================================================

which(colnames(modechoice.training)=="weather1") #4
which(colnames(modechoice.training)=="fare3") #18
#columns 4 to 17 contains attributes for the 3 alternatives

# Checking the distribution of stated choices in the training and test sets:
plot(modechoice.training$A)
plot(modechoice.test$A)
plot(modechoice$A)


# Creating an mlogit object ==================================================

S <- mlogit.data(modechoice.training,
                 shape="wide",
                 choice="A",
                 varying=c(4:18),
                 sep="",
                 alt.levels=c("A.bus","A.cyclingPMD","A.saav"),
                 id.var="Case")

summary(S)

# This function takes in a model and outputs the AIC value =====================
# AIC = -2*Loglikelihood + 2*(num.params+intercept)
# Lower AIC is better

calc.AIC <- function (model) {
  return(-2*model$logLik[1] + 2*length(model$coefficients))
}

#mode related variables

M1 <- mlogit(A ~ 1|weather,data=S)
summary(M1)
AIC(M1) #1357.116

M2 <- mlogit(A ~ fare|weather, 
            data = S) 
summary(M2)
AIC(M2) #1333.944

M3 <- mlogit(A ~ fare+wait|weather, 
             data = S) 
summary(M3)
AIC(M3) #1334.169 but mcfadden rsq increased slightly

M4 <- mlogit(A ~ fare+wait+ivvt|weather, 
             data = S) 
summary(M4)
AIC(M4) #1335.254 but mcfadden rsq increased slightly

#conclusion: M2 is the better model due to lower AIC, but if i were to include wait, i would take M4 due to how the p value when wait+ivvt is lower than in just wait

# now we include person attributes too building upon M2

M5 <- mlogit(A ~ fare|weather+Age, 
             data = S) 
summary(M5)
AIC(M5) #1326.97 
#age sig for cycling

M6 <- mlogit(A ~ fare|weather+Age+Gender, 
             data = S) 
summary(M6)
AIC(M6) #1328.847 
#AIC increased so gender is not to be added

M7 <- mlogit(A ~ fare|weather+Age+HouseholdType, 
             data = S) 
summary(M7)
AIC(M7) #1320.019
#lowest AIC so far

M8 <- mlogit(A ~ fare|weather+Age+HouseholdType+HouseholdSize, 
             data = S) 
summary(M8)
AIC(M8) #1323.093
#either householdtype or householdsize should stay

M9 <- mlogit(A ~ fare|weather+Age+HouseholdSize, 
             data = S) 
summary(M9)
AIC(M9) #1328.914
#householdsize better than householdtype

M10 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Residential, 
             data = S) 
summary(M10)
AIC(M10) #1324.68
#not very significant compared to the rest of the variables so take out residential

#add in income
M11 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income, 
              data = S) 
summary(M11)
AIC(M11) #1318.375
#lowest AIC so far

M12 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income+Cars, 
              data = S) 
summary(M12)
AIC(M12) #1319.543
#cars coeff not sig at all

M12 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M12)

AIC(M12) #1318.999
#concession was not stated for SAAV though

#now we try adding the other mode variables that we removed just now

M13 <- mlogit(A ~ fare+wait|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M13)
AIC(M13) #1319.172
#not better than M11

M14 <- mlogit(A ~ fare+wait+ivvt|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M14)
AIC(M14) #1320.237
#ivvt not sig at all and not better than M11

M15 <- mlogit(A ~ fare+ivvt|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M15)
AIC(M15) #1320.235
#ivvt still not sig anw

#conclusion: M11 is the best due to lowest AIC and highest proportion of significant coeffs

# now we include person attributes too building upon M4

M16 <- mlogit(A ~ fare+wait+ivvt|weather+Age, 
             data = S) 
summary(M16)
AIC(M16) #1328.258

M17 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender, 
              data = S) 
summary(M17)
AIC(M17) #1330.134

M18 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType, 
              data = S) 
summary(M18)
AIC(M18) #1322.806

M19 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+HouseholdSize, 
              data = S) 
summary(M19)
AIC(M19) #1325.969

M20 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdSize, 
              data = S) 
summary(M20)
AIC(M20) #1332.142

M21 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+Income, 
              data = S) 
summary(M21)
AIC(M21) #1311.476

M22 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+Income+Cars, 
              data = S) 
summary(M22)
AIC(M22) #1308.912

M23 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+Income+Cars+Concession, 
              data = S) 
summary(M23)
AIC(M23) #1309.843

#conclusion: use M23 due to lowest AIC 

Likelihoodratio <- 1-(-650.87/(696*log(1/3)))
Likelihoodratio

P <- predict(M23,newdata=S)
P
length(P)

PredictedChoice <- apply(P,1,which.max)
PredictedChoice
length(PredictedChoice)

ActualChoice <- modechoice.training$A
length(ActualChoice)

table(PredictedChoice, ActualChoice)
Accuracy11 <- (58+40+291)/696
Accuracy11

Accuracy23 <- (63+43+278)/696 
Accuracy23

Accuracy22<- (68+42+282)/696
Accuracy22

#highest accuracy 0.5632184

# A McFadden's pseudo R-squared ranging from 0.2 to 0.4 indicates very good model fit.

cor(modechoice[,33:40])

save(M23, file="M23.rda")
sample_data = S[1:3,]
save(sample_data, file = "sample_data.rData")

topredict = S[1:3,]
#topredict = S[1:3,c(4, 18:20, 23:25, 26:31)]

predict(M23, newdata = sample_data, type="response")

#M23$coefficients["A.saav:Age"]

```


---
title: "Multinomial Logit Model (MNL)"
output: 
  html_document:
      toc: true
      toc_float: true
      toc_depth: 4
      theme: readable
      css: report_styles.css
      df_print: paged
---

<hr>

<br></br>

## 1 Introduction

This document elaborates on the steps done to run Multinomial Logistic Regression on the results of the <a href="statedpref.html">Stated Preference Survey</a>. Multinomial Logistic Regression serves to predict an individual's mode choice (public bus, shared bicycles/PMDs, or Dalphin) using inputs such as the individual's demographics, mode specific variables and non-mode specific variables.

<br></br>

## 2 The Data

First, we read in the data from the stated preference survey which is in the CSV format (Comma Separated Values).

```{r results = 'hide'}
modechoice <- read.csv("ModeChoice.csv")
```

Looking at the structure of the dataset `str()`, it contains 1000 observations of 40 variables.

```{r results='hide'}
str(modechoice)
```

<br></br>

<hr>

## 3 Predicting "NA" Incomes with Linear Regression

Some of the entries in the `Income` column are `NA`. To fill this missing information, we used linear regression to predict their values.

We separate the dataset into two: one with stated incomes, and another which has `NA` for `Income`.

```{r}
incomenotNA <- subset(modechoice, modechoice$Income != "NA")
incomeNA <- subset(modechoice,is.na(modechoice$Income))
```

There are `r dim(incomenotNA)[1]` observations with stated incomes and `r dim(incomeNA)[1]` observations where income is `NA`. 

<br></br>

#### 3.1 Creating Training and Test Sets

We now create training and test sets from the observations where incomes are stated (`incomenotNA`). The `caTools` package is used.

```{r warning=FALSE, message=FALSE}
library(caTools)
```

We set the random seed so that the results will be replicable. 

```{r}
set.seed(1)
```

We use the `sample.split()` function to obtain the training and test sets with a 65/35 split ratio. Both income training and test sets maintained the same proportion of `Age` variable as in the known income dataset, `incomenotNA`.

```{r}
split <- sample.split(incomenotNA$Age, SplitRatio = 0.65)

incometraining <- subset(incomenotNA,split==TRUE)
incometest <- subset(incomenotNA,split==FALSE)
```

<br></br>

#### 3.2 Creating a Linear Regression Model

We now create a linear regression model with the training set. The first model includes all demographic information of the individual:

Variable          Description
-------------     -----------------------
`Age`             The individual's age group
`Gender`          The individual's gender
`HouseholdType`   Household type
`HouseholdSize`   Number of people in the household
`Residential`     Residential location of the household
`Cars`            The number of cars in the household
`Concession`      The type of concession pass, if any
-------------     -------------------

```{r}
income.model1 <- lm(Income ~ Age + Gender + HouseholdType + 
                      HouseholdSize + Residential + Cars + 
                      Concession,
                    data = incometraining)

summary(income.model1)
```

The model has an adjusted $R^2$ value of `r round(summary(income.model1)$adj.r.squared, 5)`. `HouseholdType` and `Residential` are not significant at the 90% significance level.

<br></br>

#### 3.3 Removing `HouseholdType` and `Residential`

We remove the insignificant variables from the model.

```{r}
income.model2 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession,
                    data=incometraining)

summary(income.model2)
```

This model has an increased adjusted $R^2$ value of `r round(summary(income.model2)$adj.r.squared, 5)`. `Concession` and `(Intercept)` are the least significant variables.

<br></br>

#### 3.4 Variable Selection

Several other combinations of variables were tested. The combination of variables which gave the highest adjusted $R^2$ was chosen.

```{r results = 'hide', echo=FALSE}
#without concession
income.model3 <- lm(Income ~ Age + Gender +HouseholdSize + Cars,
                    data=incometraining)

summary(income.model3)
#Adjusted R-squared:  0.3955

#add back concession, and household type
income.model4 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession + HouseholdType,
                    data=incometraining)

summary(income.model4)
#Adjusted R-squared:  0.3979 

cor(modechoice$Residential,modechoice$HouseholdType)

#add back concession, and household type
income.model5 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession + Residential,
                    data=incometraining)

summary(income.model5)
#Adjusted R-squared:  0.398 
```


|Model|`Age`|`Gender`|`HouseholdType`|`HouseholdSize`|`Residential`|`Cars`|`Concession`|Adjusted $R^2$|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---------------|
|1    |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|`r round(summary(income.model1)$adj.r.squared, 5)`|
|**2**|$\checkmark$|$\checkmark$|            |$\checkmark$|            |$\checkmark$|$\checkmark$|**`r round(summary(income.model2)$adj.r.squared, 5)`**|
|3    |$\checkmark$|$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |`r round(summary(income.model3)$adj.r.squared, 5)`|
|4    |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|`r round(summary(income.model4)$adj.r.squared, 5)`|
|5    |$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|`r round(summary(income.model5)$adj.r.squared, 5)`|

Model 2 has the highest adjusted $R^2$. We now evaluate this model on the test set by calculating the test $R^2$. This is given by the formula:

$$R^2 = 1 - \frac{SSE}{SST}$$

where $SSE$  is the Sum of Squared Errors and $SST$ is the Sum of Squared Errors (Total).

```{r}
incomeprediction <- predict(income.model2, newdata=incometest)
sse <- sum((incomeprediction-incometest$Income)^2)
sst <- sum((incometest$Income-mean(incometraining$Income))^2)
```

The test $R^2$ for model 2 is `r round(1-sse/sst, 5)`. We now use model 2 to predict the `NA` entries in the dataset.

```{r}
NAincomeprediction <- predict(income.model2, newdata=incomeNA) 

modechoice$Income <- ifelse(is.na(modechoice$Income),
                            NAincomeprediction,
                            modechoice$Income)
```

<br></br>

<hr>

## 4 Fitting the MNL model

Now that all entries in the dataset have no `NA` values, we can proceed to conduct Multinomial Logistic Regression.

<br></br>

#### 4.1 Generating Training and Test Sets

Each row in the dataset corresponds to a choice task of a particular individual. We  randomly split the 125 individuals into the training or test set with a 70/30 split ratio, while keeping all 8 choice tasks intact.

```{r}
set.seed(0)
mask = rep(x = sample.split(rep(1, 125), SplitRatio = 0.7),
           each = 8)
modechoice.training = subset(modechoice, mask==TRUE)
modechoice.test = subset(modechoice, mask==FALSE)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
```

The table below shows the first 16 rows of the training set. All 8 choice tasks of an individual are preserved. `Case` refers to the individual, while `Task` refers to the choice task.

```{r}
kable(modechoice.training[1:16,2:6], row.names = FALSE)
```

<br></br>

#### 4.2 Creating an `mlogit()` Object

We use the `mlogit` package which will help us conduct Multinomial Logistic Regression.

```{r message=FALSE, warning=FALSE}
library(mlogit)
```

```{r eval=FALSE}
S <- mlogit.data(data = modechoice.training,
                 shape="wide",
                 choice="A",
                 varying=c(4:18),
                 sep="",
                 alt.levels=c("A.bus","A.cyclingPMD","A.dalphin"),
                 id.var="Case")
```

```{r echo=FALSE, result='hide'}
S <- mlogit.data(data = modechoice.training,
                 shape="wide",
                 choice="A",
                 varying=c(4:18),
                 sep="",
                 alt.levels=c("A.bus","A.cyclingPMD","A.saav"),
                 id.var="Case")
```

The `mlogit.data()` function shapes the data into a suitable form for the use of the `mlogit()` function. There are several parameters to be defined in this function:

* `data` specifies the dataset to be used. Here, we use the training set created previously.

* `varying` refers to the columns that correspond to the attributes of the choice.

* `alt.levels` specifies a list of the choices available (Public bus, Cycling/PMDs, or Dalphin).

<br></br>

#### 4.3 Calculating AIC

The Akaike Information Criterion (AIC) is a measure of the quality of fit of a model which also takes model complexity into account. A lower AIC value is more desirable. The formula is given by:

$$AIC = -2 LL(\hat\beta) + 2|\hat\beta|$$

where $LL(\hat\beta)$ is the Log-likelihood of the estimated model and $|\hat\beta|$ is the number of coefficients to be estimated (including the intercept).

For convenience, we define a function `calc.AIC()` that takes in a model and returns its AIC value.

```{r}
calc.AIC <- function (model) {
  return(-2*model$logLik[1] + 2*length(model$coefficients))
}
```

```{r echo=FALSE, results='hide'}
#mode related variables

M1 <- mlogit(A ~ 1|weather,data=S)
summary(M1)
AIC(M1) #1357.116

M2 <- mlogit(A ~ fare|weather, 
            data = S) 
summary(M2)
AIC(M2) #1333.944

M3 <- mlogit(A ~ fare+wait|weather, 
             data = S) 
summary(M3)
AIC(M3) #1334.169 but mcfadden rsq increased slightly

M4 <- mlogit(A ~ fare+wait+ivvt|weather, 
             data = S) 
summary(M4)
AIC(M4) #1335.254 but mcfadden rsq increased slightly

#conclusion: M2 is the better model due to lower AIC, but if i were to include wait, i would take M4 due to how the p value when wait+ivvt is lower than in just wait

# now we include person attributes too building upon M2

M5 <- mlogit(A ~ fare|weather+Age, 
             data = S) 
summary(M5)
AIC(M5) #1326.97 
#age sig for cycling

M6 <- mlogit(A ~ fare|weather+Age+Gender, 
             data = S) 
summary(M6)
AIC(M6) #1328.847 
#AIC increased so gender is not to be added

M7 <- mlogit(A ~ fare|weather+Age+HouseholdType, 
             data = S) 
summary(M7)
AIC(M7) #1320.019
#lowest AIC so far

M8 <- mlogit(A ~ fare|weather+Age+HouseholdType+HouseholdSize, 
             data = S) 
summary(M8)
AIC(M8) #1323.093
#either householdtype or householdsize should stay

M9 <- mlogit(A ~ fare|weather+Age+HouseholdSize, 
             data = S) 
summary(M9)
AIC(M9) #1328.914
#householdsize better than householdtype

M10 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Residential, 
             data = S) 
summary(M10)
AIC(M10) #1324.68
#not very significant compared to the rest of the variables so take out residential

#add in income
M11 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income, 
              data = S) 
summary(M11)
AIC(M11) #1318.375
#lowest AIC so far

M12 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income+Cars, 
              data = S) 
summary(M12)
AIC(M12) #1319.543
#cars coeff not sig at all

M13 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M13)

AIC(M13) #1318.999
#concession was not stated for SAAV though

#now we try adding the other mode variables that we removed just now

M14 <- mlogit(A ~ fare+wait|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M14)
AIC(M14) #1319.172
#not better than M11

M15 <- mlogit(A ~ fare+wait+ivvt|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M15)
AIC(M15) #1320.237
#ivvt not sig at all and not better than M11

M16 <- mlogit(A ~ fare+ivvt|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M16)
AIC(M16) #1320.235
#ivvt still not sig anw

#conclusion: M11 is the best due to lowest AIC and highest proportion of significant coeffs

# now we include person attributes too building upon M4

M17 <- mlogit(A ~ fare+wait+ivvt|weather+Age, 
             data = S) 
summary(M17)
AIC(M17) #1328.258

M18 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender, 
              data = S) 
summary(M18)
AIC(M18) #1330.134

M19 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType, 
              data = S) 
summary(M19)
AIC(M19) #1322.806

M20 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+HouseholdSize, 
              data = S) 
summary(M20)
AIC(M20) #1325.969

M21 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdSize, 
              data = S) 
summary(M21)
AIC(M21) #1332.142

M22 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+Income, 
              data = S) 
summary(M22)
AIC(M22) #1311.476

M23 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+Income+Cars, 
              data = S) 
summary(M23)
AIC(M23) #1308.912
```

<br></br>

#### 4.4 Variable Selection

There are three broad types of variable selection:

* _Forward_: Add one variable at a time. Remove this variable if it is not significant. If significant, add another variable.

* _Backward_: Add all variables into model. Remove least significant variable each time. 

* _Stepwise_:  Combination of the forward and backward selection techniques.  After each step in which a variable was added, all variables in the model are checked to see if their significance has been reduced below the threshold significance level. If a nonsignificant variable is found, it is removed from the model. A variable can be added or removed from the previous model.

The stepwise variable selection was executed for its thoroughness.

```{r echo=FALSE, results='hide'}

models = data.frame(matrix(nrow = 23, ncol = 15))
colnames(models) = c("model", "weather", "fare", "wait", "ivtt", "access", "Age", "Gender", "Income", "HouseholdType", "HouseholdSize", "Residential", "Cars", "Concession", "AIC Value")

models[1,] = c(1, "X", "", "", "", "", "", "", "", "", "", "", "", "", round(calc.AIC(M1), 3))
models[2,] = c(2, "X", "X", "", "", "", "", "", "", "", "", "", "", "", round(calc.AIC(M2), 3))
models[3,] = c(3, "X", "X", "X", "", "", "", "", "", "", "", "", "", "", round(calc.AIC(M3), 3))
models[4,] = c(4, "X", "X", "X", "X", "", "", "", "", "", "", "", "", "", round(calc.AIC(M4), 3))
models[5,] = c(5, "X", "X", "", "", "", "X", "", "", "", "", "", "", "", round(calc.AIC(M5), 3))
models[6,] = c(6, "X", "X", "", "", "", "X", "X", "", "", "", "", "", "", round(calc.AIC(M6), 3))
models[7,] = c(7, "X", "X", "", "", "", "X", "", "", "X", "", "", "", "", round(calc.AIC(M7), 3))
models[8,] = c(8, "X", "X", "", "", "", "X", "", "", "X", "X", "", "", "", round(calc.AIC(M8), 3))
models[9,] = c(9, "X", "X", "", "", "", "X", "", "", "", "X", "", "", "", round(calc.AIC(M9), 3))
models[10,] = c(10, "X", "X", "", "", "", "X", "", "", "", "X", "X", "", "", round(calc.AIC(M10), 3))
models[11,] = c(11, "X", "X", "", "", "", "X", "", "X", "", "X", "", "", "", round(calc.AIC(M11), 3))
models[12,] = c(12, "X", "X", "", "", "", "X", "", "X", "", "X", "", "X", "", round(calc.AIC(M12), 3))
models[13,] = c(13, "X", "X", "", "", "", "X", "", "X", "", "X", "", "", "X", round(calc.AIC(M13), 3))
models[14,] = c(14, "X", "X", "X", "", "", "X", "", "X", "", "X", "", "", "X", round(calc.AIC(M14), 3))
models[15,] = c(15, "X", "X", "X", "X", "", "X", "", "X", "", "X", "", "", "X", round(calc.AIC(M15), 3))
models[16,] = c(16, "X", "X", "", "X", "", "X", "", "X", "", "X", "", "", "X", round(calc.AIC(M16), 3))

models[17,] = c(17, "X", "X", "X", "X", "", "X", "", "", "", "", "", "", "", round(calc.AIC(M17), 3))
models[18,] = c(18, "X", "X", "X", "X", "", "X", "X", "", "", "", "", "", "", round(calc.AIC(M18), 3))
models[19,] = c(19, "X", "X", "X", "X", "", "X", "X", "", "X", "", "", "", "", round(calc.AIC(M19), 3))
models[20,] = c(20, "X", "X", "X", "X", "", "X", "X", "", "X", "X", "", "", "", round(calc.AIC(M20), 3))
models[21,] = c(21, "X", "X", "X", "X", "", "X", "X", "", "", "X", "", "", "", round(calc.AIC(M21), 3))
models[22,] = c(22, "X", "X", "X", "X", "", "X", "X", "X", "X", "", "", "", "", round(calc.AIC(M22), 3))
models[23,] = c(23, "X", "X", "X", "X", "", "X", "X", "X", "X", "", "", "X", "", round(calc.AIC(M23), 3))
```

```{r echo=FALSE, results="asis"}
kable(as.matrix(models[1:4,]), format = "html", row.names = FALSE) %>%
  kable_styling(bootstrap_options = c("responsive", "condensed"),
                font_size = 13)
```

<!--
|Model|`weather`|`fare`|`wait`|`ivtt`|`access`|`Age`|`Gender`|`Income`|`HouseholdType`|`HouseholdSize`|`Residential`|`Cars`|`Concession`|AIC Value|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:------:|:------:|:------:|:---:|:------:|:---------|
|1    |$\checkmark$|            |            |            |            |            |            |            |            |            |            |            |            |`r round(calc.AIC(M1), 3)`|
|2    |$\checkmark$|$\checkmark$|            |            |            |            |            |            |            |            |            |            |            |`r round(calc.AIC(M2), 3)`|
|3    |$\checkmark$|$\checkmark$|$\checkmark$|            |            |            |            |            |            |            |            |            |            |`r round(calc.AIC(M3), 3)`|
|4    |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |            |            |            |            |            |            |            |            |`r round(calc.AIC(M4), 3)`|
-->

Model 2 the better model as it has the lowest AIC. We now build upon Model 2 by including individual-specific attributes.

```{r echo=FALSE}
kable(models[5:13,], format = "html", row.names = FALSE) %>%
  kable_styling(bootstrap_options = c("responsive", "condensed"),
                font_size = 13)
```

<!--
|Model|`weather`|`fare`|`wait`|`ivtt`|`access`|`Age`|`Gender`|`Income`|`HouseholdType`|`HouseholdSize`|`Residential`|`Cars`|`Concession`|AIC Value|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:------:|:------:|:------:|:---:|:------:|:---------|
|5    |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|            |            |            |            |            |            |            |`r round(calc.AIC(M5), 3)`|
|6    |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|$\checkmark$|            |            |            |            |            |            |`r round(calc.AIC(M6), 3)`|
|7    |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|            |            |$\checkmark$|            |            |            |            |`r round(calc.AIC(M7), 3)`|
|8    |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|            |            |$\checkmark$|$\checkmark$|            |            |            |`r round(calc.AIC(M8), 3)`|
|9    |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|            |            |            |$\checkmark$|            |            |            |`r round(calc.AIC(M9), 3)`|
|10   |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|            |            |            |$\checkmark$|$\checkmark$|            |            |`r round(calc.AIC(M10), 3)`|
|11   |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|            |$\checkmark$|            |$\checkmark$|$\checkmark$|            |            |`r round(calc.AIC(M11), 3)`|
|12   |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |`r round(calc.AIC(M12), 3)`|
|13   |$\checkmark$|$\checkmark$|            |            |            |$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |            |$\checkmark$|`r round(calc.AIC(M13), 3)`|
-->

Now we try adding the other mode-specific variables that we removed just now.

```{r echo=FALSE}
kable(models[14:16,], format = "html", row.names = FALSE) %>%
  kable_styling(bootstrap_options = c("responsive", "condensed"),
                font_size = 13)
```

<!--
|Model|`weather`|`fare`|`wait`|`ivtt`|`access`|`Age`|`Gender`|`Income`|`HouseholdType`|`HouseholdSize`|`Residential`|`Cars`|`Concession`|AIC Value|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:------:|:------:|:------:|:---:|:------:|:---------|
|14   |$\checkmark$|$\checkmark$|$\checkmark$|            |            |$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |            |$\checkmark$|`r round(calc.AIC(M14), 3)`|
|15   |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |            |$\checkmark$|`r round(calc.AIC(M15), 3)`|
|16   |$\checkmark$|$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |$\checkmark$|            |            |$\checkmark$|`r round(calc.AIC(M16), 3)`|
-->

Model 11 is the best as it has the lowest AIC and highest proportion of significant coefficients. Now we build upon Model 4 by including individual-specific attributes.

```{r echo=FALSE}
kable(models[17:23,], format = "html", row.names = FALSE) %>%
  kable_styling(bootstrap_options = c("responsive", "condensed"),
                font_size = 13)
```


<!--
|Model|`weather`|`fare`|`wait`|`ivtt`|`access`|`Age`|`Gender`|`Income`|`HouseholdType`|`HouseholdSize`|`Residential`|`Cars`|`Concession`|AIC Value|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:------:|:------:|:------:|:---:|:------:|:---------|
|17   |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|            |            |            |            |            |            |            |`r round(calc.AIC(M17), 3)`|
|18   |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|            |            |            |            |            |            |`r round(calc.AIC(M18), 3)`|
|19   |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|            |$\checkmark$|            |            |            |            |`r round(calc.AIC(M19), 3)`|
|20   |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|            |            |            |`r round(calc.AIC(M20), 3)`|
|21   |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|            |            |$\checkmark$|            |            |            |`r round(calc.AIC(M21), 3)`|
|22   |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |            |            |            |`r round(calc.AIC(M22), 3)`|
|23   |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|            |            |$\checkmark$|            |`r round(calc.AIC(M23), 3)`|
-->

Overall, Model 23 is the best model with the lowest AIC value.

This is the full table of all the models constructed.

```{r echo=FALSE}
kable(models, format = "html", row.names = FALSE) %>%
  kable_styling(bootstrap_options = c("responsive", "condensed"),
                font_size = 13)
```

<br></br>

#### 4.5 Predicting Mode Choice

We use the `predict()` function to give the probability of an individual choosing a particular mode.

```{r}
P <- predict(M23, newdata = S, type = "response")
```

```{r echo=FALSE, results='hide'}
colnames(P) = c("Bus", "Shared Bicycles/PMDs", "Dalphin")
```

Looking at the first three rows of the dataset:

```{r results='hide', echo=FALSE}
P <- round(P, 3)
```

```{r echo=FALSE}
kable(P[1:3,], format = "html", align = 'c') %>%
  kable_styling(bootstrap_options = c("striped"))
```

For example, the individual corresponding to the first row will choose the bus with a `r round(P[1,1]*100, 1)`% chance, Shared Bicycles/PMDs with a `r round(P[1,2]*100, 1)`% chance, and the Dalphin with a `r round(P[1,3]*100, 1)`% chance.

We let the predicted choice to be the most likely alternative (highest chance of being chosen).

```{r}
PredictedChoice <- apply(P, 1, which.max)
```

Using a confusion matrix, we compare the actual choice in the training set with the predicted choice by the model. 

```{r results='hide', echo=FALSE}
PredictedChoice = as.factor(PredictedChoice)
levels(PredictedChoice) = c("Bus", "Shared Bicycles/PMDs", "Dalphin")
```

```{r}
ActualChoice <- modechoice.training$A
```

```{r results='hide', echo=FALSE}
levels(ActualChoice) = c("Bus", "Shared Bicycles/PMDs", "Dalphin")
```

**Predicted Choice vs. Actual Choice**

```{r echo=FALSE}
kable(table(PredictedChoice, ActualChoice), format = "html") %>%
  kable_styling(bootstrap_options = c("responsive")) %>%
  add_header_above(c(" " = 1, "Actual Choice" = 3))
```


The diagonal entries of the confusion matrix are the instances where the model correctly predicts the actual choice of the individual. Accuracy is calculated by finding the proportion of instances where the model correctly predicted the actual choice. This model has an accuracy of `r round(((68+42+282)/696)*100, 2)`%.

<br></br>

<hr>

```{r eval=FALSE, echo=FALSE}
library(mlogit)
library(caTools)


#    MODE CHOICE STATED PREFERENCE SURVEY

# The Data ==============================================================


hist(modechoice$Age)

withSAAV <- modechoice[,c(1:21,25:39)]
noSAAV <- modechoice[,c(1:17,22:39)]

#install.packages("mlogit")
library(mlogit)

# Estimating income for refused / unknown using linear regression ============
  
incomenotNA <- subset(modechoice, modechoice$Income != "NA")
str(incomenotNA)
incomeNA <- subset(modechoice,is.na(modechoice$Income))
str(incomeNA)

#install.packages("caTools")

set.seed(1)
split <- sample.split(incomenotNA$Age, SplitRatio = 0.65)
#splits dataset into .65 and .35 with proportion of TENCHD in each being same as it was in framing1

incometraining <- subset(incomenotNA,split==TRUE)
incometest <- subset(incomenotNA,split==FALSE)
str(incometraining) #larger set
str(incometest) #smaller set

#summary(step(income.model1))

income.model1 <- lm(Income ~ Age + Gender + HouseholdType + 
                      HouseholdSize + Residential + Cars + 
                      Concession,
                    data=incometraining)

summary(income.model1)

#Adjusted R-squared:  0.3968
#but household type and residential not sig at all

#take out hsehold type and residential
income.model2 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession,
                    data=incometraining)

summary(income.model2)
#Adjusted R-squared:  0.3991 
#least sig is int and concession

#without concession
income.model3 <- lm(Income ~ Age + Gender +HouseholdSize + Cars,
                    data=incometraining)

summary(income.model3)
#Adjusted R-squared:  0.3955

#add back concession, and household type
income.model4 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession + HouseholdType,
                    data=incometraining)

summary(income.model4)
#Adjusted R-squared:  0.3979 

cor(modechoice$Residential,modechoice$HouseholdType)

#add back concession, and household type
income.model5 <- lm(Income ~ Age + Gender +HouseholdSize + Cars + Concession + Residential,
                    data=incometraining)

summary(income.model5)
#Adjusted R-squared:  0.398 


plot(incomeprediction, jitter(incometest$Income, 1),
     cex = 0.5,
     xlim = c(1,5),
     ylim = c(1, 5))

abline(a=0, b=1)

incomeprediction <- predict(income.model2, newdata=incometest)
sse <- sum((incomeprediction-incometest$Income)^2)
#sse is the sum of square of difference between the true value and the estimated value

sst <- sum((incometest$Income-mean(incometraining$Income))^2)

#sst is the sum of square of diff between the true value and the mean of the true values
1-sse/sst
#this is the R^2
#0.3740837


#cor(modechoice)
# Generating training and test sets ===========================================
library(caTools)

set.seed(0)

mask = rep(x = sample.split(rep(1, 125), SplitRatio = 0.7),
           each = 8)

modechoice.training = subset(modechoice, mask==TRUE)

modechoice.test = subset(modechoice, mask==FALSE)

# ============================================================================

which(colnames(modechoice.training)=="weather1") #4
which(colnames(modechoice.training)=="fare3") #18
#columns 4 to 17 contains attributes for the 3 alternatives

# Checking the distribution of stated choices in the training and test sets:
plot(modechoice.training$A)
plot(modechoice.test$A)
plot(modechoice$A)


# Creating an mlogit object ==================================================

S <- mlogit.data(modechoice.training,
                 shape="wide",
                 choice="A",
                 varying=c(4:18),
                 sep="",
                 alt.levels=c("A.bus","A.cyclingPMD","A.saav"),
                 id.var="Case")

summary(S)

# This function takes in a model and outputs the AIC value =====================
# AIC = -2*Loglikelihood + 2*(num.params+intercept)
# Lower AIC is better

calc.AIC <- function (model) {
  return(-2*model$logLik[1] + 2*length(model$coefficients))
}

#mode related variables

M1 <- mlogit(A ~ 1|weather,data=S)
summary(M1)
AIC(M1) #1357.116

M2 <- mlogit(A ~ fare|weather, 
            data = S) 
summary(M2)
AIC(M2) #1333.944

M3 <- mlogit(A ~ fare+wait|weather, 
             data = S) 
summary(M3)
AIC(M3) #1334.169 but mcfadden rsq increased slightly

M4 <- mlogit(A ~ fare+wait+ivvt|weather, 
             data = S) 
summary(M4)
AIC(M4) #1335.254 but mcfadden rsq increased slightly

#conclusion: M2 is the better model due to lower AIC, but if i were to include wait, i would take M4 due to how the p value when wait+ivvt is lower than in just wait

# now we include person attributes too building upon M2

M5 <- mlogit(A ~ fare|weather+Age, 
             data = S) 
summary(M5)
AIC(M5) #1326.97 
#age sig for cycling

M6 <- mlogit(A ~ fare|weather+Age+Gender, 
             data = S) 
summary(M6)
AIC(M6) #1328.847 
#AIC increased so gender is not to be added

M7 <- mlogit(A ~ fare|weather+Age+HouseholdType, 
             data = S) 
summary(M7)
AIC(M7) #1320.019
#lowest AIC so far

M8 <- mlogit(A ~ fare|weather+Age+HouseholdType+HouseholdSize, 
             data = S) 
summary(M8)
AIC(M8) #1323.093
#either householdtype or householdsize should stay

M9 <- mlogit(A ~ fare|weather+Age+HouseholdSize, 
             data = S) 
summary(M9)
AIC(M9) #1328.914
#householdsize better than householdtype

M10 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Residential, 
             data = S) 
summary(M10)
AIC(M10) #1324.68
#not very significant compared to the rest of the variables so take out residential

#add in income
M11 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income, 
              data = S) 
summary(M11)
AIC(M11) #1318.375
#lowest AIC so far

M12 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income+Cars, 
              data = S) 
summary(M12)
AIC(M12) #1319.543
#cars coeff not sig at all

M13 <- mlogit(A ~ fare|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M13)

AIC(M13) #1318.999
#concession was not stated for SAAV though

#now we try adding the other mode variables that we removed just now

M14 <- mlogit(A ~ fare+wait|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M14)
AIC(M14) #1319.172
#not better than M11

M15 <- mlogit(A ~ fare+wait+ivvt|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M15)
AIC(M15) #1320.237
#ivvt not sig at all and not better than M11

M16 <- mlogit(A ~ fare+ivvt|weather+Age+HouseholdSize+Income+Concession, 
              data = S) 
summary(M16)
AIC(M16) #1320.235
#ivvt still not sig anw

#conclusion: M11 is the best due to lowest AIC and highest proportion of significant coeffs

# now we include person attributes too building upon M4

M17 <- mlogit(A ~ fare+wait+ivvt|weather+Age, 
             data = S) 
summary(M17)
AIC(M17) #1328.258

M18 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender, 
              data = S) 
summary(M18)
AIC(M18) #1330.134

M19 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType, 
              data = S) 
summary(M19)
AIC(M19) #1322.806

M20 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+HouseholdSize, 
              data = S) 
summary(M20)
AIC(M20) #1325.969

M21 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdSize, 
              data = S) 
summary(M21)
AIC(M21) #1332.142

M22 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+Income, 
              data = S) 
summary(M22)
AIC(M22) #1311.476

M23 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+Income+Cars, 
              data = S) 
summary(M23)
AIC(M23) #1308.912

# Cut here....
M24 <- mlogit(A ~ fare+wait+ivvt|weather+Age+Gender+HouseholdType+Income+Cars+Concession, 
              data = S) 
summary(M24)
AIC(M24) #1309.843
# Cut here....
#conclusion: use M23 due to lowest AIC 
# Use M23 actually.

Likelihoodratio <- 1-(-650.87/(696*log(1/3)))
Likelihoodratio

P <- predict(M23,newdata=S)
P
length(P)

PredictedChoice <- apply(P,1,which.max)
PredictedChoice
length(PredictedChoice)

ActualChoice <- modechoice.training$A
length(ActualChoice)

table(PredictedChoice, ActualChoice)
Accuracy11 <- (58+40+291)/696
Accuracy11

Accuracy23 <- (63+43+278)/696 
Accuracy23

Accuracy22<- (68+42+282)/696
Accuracy22

#highest accuracy 0.5632184

# A McFadden's pseudo R-squared ranging from 0.2 to 0.4 indicates very good model fit.

cor(modechoice[,33:40])

save(M23, file="M23.rda")
sample_data = S[1:3,]
save(sample_data, file = "sample_data.rData")

topredict = S[1:3,]
#topredict = S[1:3,c(4, 18:20, 23:25, 26:31)]

predict(M23, newdata = sample_data, type="response")

#M23$coefficients["A.saav:Age"]

```

